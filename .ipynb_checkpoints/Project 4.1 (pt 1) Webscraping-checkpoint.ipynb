{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "        \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%run helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This doesn't really need a summary but i can say that finding data straight from the real world is now very easy and intuitive \n",
    "although there will be alot of messiness and it won't be the most useful data without heavy cleaning \n",
    "\n",
    "For this project the Scraping for jobs was done in a general sense which means that i utilised general search terms to find all data related jobs rather than using specific words\n",
    "Reason being is that this would encompass all data related jobs and i won't miss out on any jobs just because of specifying\n",
    "as a result a i got 30312 jobs\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i used only 6 search parameters is that i believed that these phrases were enough to cover all data-related jobs from all fields\n",
    "\n",
    "also i have set it up in a manner that i can re-use the same code for different job fields and even specific search terms in the future without significant or no change to the code.(just change the search terms and BoB's your Uncle)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I used 12 different features that i thought would we useful in the analysis and questions that they might be able to answer:\n",
    "\n",
    "1)title: This is neccessary for the project itself\n",
    "\n",
    "2)salary: agan it is neccessary \n",
    "\n",
    "3)location: where are the data related jobs and where the higher salaries might be\n",
    "\n",
    "4)area(specific area within location): do specific areas withnin each location impact title and salaries\n",
    "\n",
    "5)class(industry): does the industry determine the salary and titles\n",
    "\n",
    "6)sub-class(sub-industry): does the sub-industry play a significant role or not \n",
    "\n",
    "7)type: is there a title or salary difference in terms of Fulltime,parttime,etc\n",
    "\n",
    "8)date: does the dates of the jobs being posted impact the salary and title\n",
    "\n",
    "9)description: what kind of description exists for data scientist roles and does it impact salary\n",
    "\n",
    "10)job_id :this is only there to get rid of duplicate jobs\n",
    "\n",
    "11)level: this is a recommendation job by seek themselves and does this make a difference or not\n",
    "\n",
    "12)company: there are many companies out there and help determine which company post what kind of jobs and the amount of salary\n",
    "\n",
    "\n",
    "Now i know that i probabaly won't be able to answer all questions and that not all features would be useful but i will need to figure that out by doing some analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the search parameters that i will be searching in the website\n",
    "#and i can keep extending list if i want to or specify it \n",
    "\n",
    "search_parameters=['informatics']#informatics doesn't have that many jobs so i used it as a test subject \n",
    "\n",
    "# the search parameters i used in my actual scraping:\n",
    "# ['data', 'analysis', 'research','informatics','statistic','business intelligence']\n",
    "\n",
    "\n",
    "#converting the search parameters to use in the url, meaing i will be placing the search terms in the url itself:\n",
    "def replace(lists):\n",
    "    for index, item in enumerate(lists):\n",
    "        lists[index]=(item.replace(' ','-'))\n",
    "    return lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['informatics']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace(search_parameters)\n",
    "\n",
    "# converted list\n",
    "search_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "500\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Now for the main course:\n",
    "# here i will be searching all the search parameters and getting the jobs for each search parameter\n",
    "\n",
    "\n",
    "\n",
    "# creating a dataframe and its index \n",
    "# and a temporary dictioonary\n",
    "data=None\n",
    "index=1 \n",
    "temp_article={}\n",
    "\n",
    "# looping through the search parameters and placing in in the url to get the jobs for each search parameter\n",
    "for item in search_parameters:\n",
    "    \n",
    "    print item #this will give me a count and progress info of the scraping\n",
    "    \n",
    "    url='https://www.seek.com.au/%s-jobs?page='%item #placing in url\n",
    "    \n",
    "    #calculating how many pages to go through by getting the job number shown(search result number) on the site and dividing it up by 20\n",
    "    # because there are 20 jobs per page\n",
    "    #however the limit is 500m pages on the website so if the search result number shown has more jobs than 500 pages \n",
    "    #it will only look for 500 pages and jobs on only 500 pages \n",
    "    \n",
    "    #getting the url for the search parameter\n",
    "    r = requests.get(url)\n",
    "    #turning it into a html doc\n",
    "    html0=html.document_fromstring(r.text)\n",
    "    #locating the 'job number'(serach result number) listed for the search parameter using xpath\n",
    "    search_page = html0.xpath('//*[@id=\"SearchSummary\"]/strong')\n",
    "    #looping through the search_page list:\n",
    "    for job_number in search_page:\n",
    "        \n",
    "        #converting the number to a usable int and rounding it up with .9 and \n",
    "        # +1 to make include the number in the range \n",
    "        num=int(round(float(job_number.text.replace(',',''))/20.0+.9))+1\n",
    "        if num>500:\n",
    "            num = 500\n",
    "        else:\n",
    "            num=num\n",
    "\n",
    "    print num  #this will give me a count and progress info of the scraping\n",
    "    \n",
    "    #looping thourgh each page\n",
    "    for page in range(1,num):\n",
    "        \n",
    "        print page #this will give me a count and progress info of the scraping\n",
    "        \n",
    "        \n",
    "        # make request for that page\n",
    "        r = requests.get(url+str(page))\n",
    "\n",
    "        # turn into a BeautifulSoup object\n",
    "        soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "\n",
    "        # find job articles on page\n",
    "        articles = soup.find_all(name='article')\n",
    "\n",
    "\n",
    "        # iterating through each job on the main search page:\n",
    "        for article in articles:\n",
    "            \n",
    "            #lookng for some feature that are not in the jobs page itself\n",
    "            #desrciption,job_id and level\n",
    "\n",
    "            #description feature:\n",
    "            dvg=article.find_all(name='span', attrs={'class':\"bl7UwXp\"})\n",
    "            for z in dvg:\n",
    "                temp_article['description']=z.text\n",
    "\n",
    "            #getting the job_id and level which an attribute of the html article code:\n",
    "            temp_article['job_id']=article.attrs['data-job-id']\n",
    "            temp_article['level']=article.attrs['data-automation']\n",
    "\n",
    "\n",
    "            #finding the url for each job so that i can go to each jobs own page\n",
    "            #and grab features straight from there\n",
    "            dvg=article.find_all(name='a',attrs={'class':\"_1EkZJQ7\"})\n",
    "            for z in dvg:\n",
    "                \n",
    "                #finding the url for the job's own page\n",
    "                s=z.attrs['href']\n",
    "                #requesting html for the job's page\n",
    "                r = requests.get('https://www.seek.com.au'+s)\n",
    "                #convering it to html file\n",
    "                html1=html.document_fromstring(r.text)\n",
    "                \n",
    "                #finding the remaining attributes on the page itself using xpath and temporarily adding it\n",
    "                #to the 'temp_article' dictionary\n",
    "\n",
    "\n",
    "                #title\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[1]/div/div/section/div/div/span/span/h1')\n",
    "                for z in dvg:\n",
    "                    temp_article['title']=z.text\n",
    "\n",
    "\n",
    "\n",
    "                #company       \n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[1]/div/div/section/div/div/h2/span[2]/span')\n",
    "                for z in dvg:\n",
    "                    temp_article['company']=z.text\n",
    "\n",
    "\n",
    "                #date\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/dd[1]/span/span')\n",
    "                for z in dvg:\n",
    "                    temp_article['date']=z.text\n",
    "\n",
    "                #location\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/dd[2]/span/span/strong')\n",
    "                for z in dvg:\n",
    "                    temp_article['location'] = z.text\n",
    "\n",
    "\n",
    "                #salary\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/div[1]/dd/span/span')\n",
    "                for z in dvg:\n",
    "                    temp_article['salary']=z.text\n",
    "\n",
    "                #area\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/dd[2]/span/span/span/text()')\n",
    "                for z in dvg:\n",
    "                    temp_article['area']=z\n",
    "\n",
    "                #type\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/dd[3]/span/span')\n",
    "                for z in dvg:\n",
    "                    temp_article['type']=z.text\n",
    "\n",
    "\n",
    "                #class\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/div/dd/span/span/strong')\n",
    "                for z in dvg:\n",
    "                    temp_article['class']=z.text\n",
    "\n",
    "                #sub_class\n",
    "                dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/div/dd/span/span/span/text()')\n",
    "                for z in dvg:\n",
    "                    temp_article['sub_class']=z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #making the dataframe using the 'temp_article' dictionary, info created at the beginning of this code\n",
    "            temp=pd.DataFrame(temp_article, index=[index])\n",
    "            index+=1\n",
    "            try:\n",
    "                data=pd.concat([data,temp])\n",
    "\n",
    "            except:\n",
    "                data=temp\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving it down as a excel file\n",
    "# also i commented it out simply because i don't want it to overlap and create a new file\n",
    "# print data.shape\n",
    "# print data.info\n",
    "# data.to_excel('./scraped.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reloaded the file to check it out once it was saved\n",
    "\n",
    "data2=pd.read_excel('./scraped.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30312, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30312 entries, 1 to 30312\n",
      "Data columns (total 12 columns):\n",
      "area           30311 non-null object\n",
      "class          30311 non-null object\n",
      "company        21172 non-null object\n",
      "date           30311 non-null object\n",
      "description    30312 non-null object\n",
      "job_id         30312 non-null int64\n",
      "level          30312 non-null object\n",
      "location       30311 non-null object\n",
      "salary         9976 non-null object\n",
      "sub_class      30311 non-null object\n",
      "title          30311 non-null object\n",
      "type           30311 non-null object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>class</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>job_id</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>sub_class</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join a talented and dynamic team in multi-chan...</td>\n",
       "      <td>35046021</td>\n",
       "      <td>premiumJob</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBD, Inner West &amp; Eastern Suburbs</td>\n",
       "      <td>Information &amp; Communication Technology</td>\n",
       "      <td>Energy &amp; Water Ombudsman NSW</td>\n",
       "      <td>19 Dec 2017</td>\n",
       "      <td>Seeking an experienced desktop support analyst...</td>\n",
       "      <td>35118800</td>\n",
       "      <td>premiumJob</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business/Systems Analysts</td>\n",
       "      <td>Data Analysis and Support Officer</td>\n",
       "      <td>Part Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBD, Inner West &amp; Eastern Suburbs</td>\n",
       "      <td>Information &amp; Communication Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21 Dec 2017</td>\n",
       "      <td>Expertise in Data Architecture frameworks e.g....</td>\n",
       "      <td>35132539</td>\n",
       "      <td>normalJob</td>\n",
       "      <td>ACT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Architects</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>Contract/Temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBD, Inner West &amp; Eastern Suburbs</td>\n",
       "      <td>Healthcare &amp; Medical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21 Dec 2017</td>\n",
       "      <td>Analyse financial, human resource, service del...</td>\n",
       "      <td>35131963</td>\n",
       "      <td>normalJob</td>\n",
       "      <td>Townsville &amp; Northern QLD</td>\n",
       "      <td>Great remuneration &amp; benefits!</td>\n",
       "      <td>Management</td>\n",
       "      <td>Finance and Data Manager</td>\n",
       "      <td>Contract/Temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CBD, Inner West &amp; Eastern Suburbs</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21 Dec 2017</td>\n",
       "      <td>Analyse financial, human resource, service del...</td>\n",
       "      <td>35131964</td>\n",
       "      <td>normalJob</td>\n",
       "      <td>Townsville &amp; Northern QLD</td>\n",
       "      <td>Great remuneration &amp; benefits!</td>\n",
       "      <td>Management</td>\n",
       "      <td>Finance and Data Manager</td>\n",
       "      <td>Contract/Temp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                area                                   class  \\\n",
       "1                                NaN                                     NaN   \n",
       "2  CBD, Inner West & Eastern Suburbs  Information & Communication Technology   \n",
       "3  CBD, Inner West & Eastern Suburbs  Information & Communication Technology   \n",
       "4  CBD, Inner West & Eastern Suburbs                    Healthcare & Medical   \n",
       "5  CBD, Inner West & Eastern Suburbs                              Accounting   \n",
       "\n",
       "                        company         date  \\\n",
       "1                           NaN          NaN   \n",
       "2  Energy & Water Ombudsman NSW  19 Dec 2017   \n",
       "3                           NaN  21 Dec 2017   \n",
       "4                           NaN  21 Dec 2017   \n",
       "5                           NaN  21 Dec 2017   \n",
       "\n",
       "                                         description    job_id       level  \\\n",
       "1  Join a talented and dynamic team in multi-chan...  35046021  premiumJob   \n",
       "2  Seeking an experienced desktop support analyst...  35118800  premiumJob   \n",
       "3  Expertise in Data Architecture frameworks e.g....  35132539   normalJob   \n",
       "4  Analyse financial, human resource, service del...  35131963   normalJob   \n",
       "5  Analyse financial, human resource, service del...  35131964   normalJob   \n",
       "\n",
       "                    location                          salary  \\\n",
       "1                        NaN                             NaN   \n",
       "2                     Sydney                             NaN   \n",
       "3                        ACT                             NaN   \n",
       "4  Townsville & Northern QLD  Great remuneration & benefits!   \n",
       "5  Townsville & Northern QLD  Great remuneration & benefits!   \n",
       "\n",
       "                   sub_class                              title           type  \n",
       "1                        NaN                                NaN            NaN  \n",
       "2  Business/Systems Analysts  Data Analysis and Support Officer      Part Time  \n",
       "3                 Architects                     Data Architect  Contract/Temp  \n",
       "4                 Management           Finance and Data Manager  Contract/Temp  \n",
       "5                 Management           Finance and Data Manager  Contract/Temp  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data2.shape\n",
    "print data2.info()\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is where is was trying alot of the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# make request for that page\n",
    "r = requests.get('https://www.seek.com.au/informatics-jobs?page='+str(1))\n",
    "\n",
    "# turn into a BeautifulSoup object\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "\n",
    "# find job article on page\n",
    "articles = soup.find_all(name='article')\n",
    "\n",
    "\n",
    "# r = requests.get(url)\n",
    "html0=html.document_fromstring(r.text)\n",
    "search_page = html0.xpath('//*[@id=\"SearchSummary\"]/strong')\n",
    "for job_number in search_page:\n",
    "    \n",
    "    num=int(round(float(job_number.text.replace(',',''))/20.0+.9))+1\n",
    "    if num>500:\n",
    "        num = 500\n",
    "    else:\n",
    "        num=num\n",
    "    print num\n",
    "    print range(1,num)\n",
    "    \n",
    "# iterating through each job:\n",
    "for article in articles:\n",
    "    \n",
    "    #description feature:\n",
    "    dvg=article.find_all(name='span', attrs={'class':\"bl7UwXp\"})\n",
    "    for z in dvg:\n",
    "        temp_article['description']=z.text\n",
    "    \n",
    "    #getting the job_id and level which an attribute of the html article code:\n",
    "    temp_article['job_id']=article.attrs['data-job-id']\n",
    "    temp_article['level']=article.attrs['data-automation']\n",
    "            \n",
    "    \n",
    "    #finding the url for each job so that i can to to each jobs page\n",
    "    #and grab features straight from there\n",
    "    dvg=article.find_all(name='a',attrs={'class':\"_1EkZJQ7\"})\n",
    "    for z in dvg:\n",
    "\n",
    "        s=z.attrs['href']\n",
    "\n",
    "        r = requests.get('https://www.seek.com.au'+s)\n",
    "\n",
    "        html1=html.document_fromstring(r.text)\n",
    "        \n",
    "        \n",
    "        #title\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[1]/div/div/section/div/div/span/span/h1')\n",
    "        for z in dvg:\n",
    "            temp_article['title']=z.text\n",
    "    \n",
    "\n",
    "\n",
    "        #company       \n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[1]/div/div/section/div/div/h2/span[2]/span')\n",
    "        for z in dvg:\n",
    "            temp_article['company']=z.text\n",
    "\n",
    "\n",
    "        #date\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/dd[1]/span/span')\n",
    "        for z in dvg:\n",
    "            temp_article['date']=z.text\n",
    "\n",
    "        #location\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/dd[2]/span/span/strong')\n",
    "        for z in dvg:\n",
    "            temp_article['location'] = z.text\n",
    "   \n",
    "\n",
    "        #salary\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/div[1]/dd/span/span')\n",
    "        for z in dvg:\n",
    "            temp_article['salary']=z.text\n",
    "\n",
    "        #area\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/dd[2]/span/span/span/text()')\n",
    "        for z in dvg:\n",
    "            temp_article['area']=z\n",
    "            \n",
    "        #type\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/dd[3]/span/span')\n",
    "        for z in dvg:\n",
    "            temp_article['type']=z.text\n",
    "            \n",
    "            \n",
    "        #class\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/aside/span/div[1]/div/section/dl/div/dd/span/span/strong')\n",
    "        for z in dvg:\n",
    "            temp_article['class']=z.text\n",
    "            \n",
    "        #sub_class\n",
    "        dvg = html1.xpath('//*[@id=\"app\"]/div/div[2]/div[2]/div/div/div[2]/div/div[2]/div/article/div/div[1]/div/section/dl/div/dd/span/span/span/text()')\n",
    "        for z in dvg:\n",
    "            temp_article['sub_class']=z\n",
    "\n",
    "    \n",
    "               \n",
    "     \n",
    "    #making the dataframe\n",
    "    temp=pd.DataFrame(temp_article, index=[index])\n",
    "    index+=1\n",
    "    try:\n",
    "        x=pd.concat([x,temp])\n",
    "        \n",
    "    except:\n",
    "        x=temp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
